Post-Training of LLMs: Transforming Your Model from Ordinary to Extraordinary

Large Language Models (LLMs) began as machines that predicted the next word. But what transformed them into assistants, teachers, coders, and creative collaborators?

The answer: Post-training.

This project explores key techniques for refining pre-trained LLMs to make them more helpful, safe, and aligned. While we'll overview several approachesâ€”including SFT, DPO, and PEFTâ€”our primary focus is on Online Reinforcement Learning (RL), the most dynamic path toward intelligent, value-aligned models.

ğŸ“š Table of Contents

Introduction

Why Post-Training?

Techniques Overview

SFT - Supervised Fine-Tuning

DPO - Direct Preference Optimization

PEFT - Parameter-Efficient Fine-Tuning

ğŸ”¬ Online Reinforcement Learning

Reward Models

Final Thoughts

Resources

License

ğŸ§  Introduction

Post-training is the essential step that transforms a raw LLM from a general-purpose word predictor into a purpose-driven, value-aware assistant. This repository and accompanying guide dive into:

The "why" behind post-training

Comparison of various fine-tuning approaches

In-depth explanation of reinforcement learning

Code resources and learning materials to get hands-on

ğŸ§± Why Post-Training?

Pre-trained models understand language, but not behavior, context, or correctness.

Think of post-training like onboarding a new hire: they speak the language but must learn the company culture.

Without post-training, LLMs may:

Hallucinate or provide incorrect information

Miss user intent

Behave unpredictably or unethically

Post-training helps align models with human values and domain-specific expectations.

ğŸ§° Techniques Overview
ğŸ¯ SFT â€” Supervised Fine-Tuning

Supervised Fine-Tuning teaches the model by example. Itâ€™s the foundation of many instruction-tuned models.

How it works:

Provide input-output pairs

Train the model to mimic ideal responses

Pros:

Easy to implement

Fast and effective for new tasks

Cons:

Limited generalization

Quality depends on the dataset

âš–ï¸ DPO â€” Direct Preference Optimization

Instead of telling the model what to do, DPO shows it what not to do.

How it works:

Compare pairs of outputs (preferred vs. non-preferred)

Use feedback to adjust behavior

Pros:

Great for removing unwanted behavior

Encourages human-like preferences

Cons:

Sensitive to biases in preference data

Implementation is more complex than SFT

ğŸ§© PEFT â€” Parameter-Efficient Fine-Tuning

Update only a small part of the model to achieve big results.

Examples: LoRA, Adapter Tuning

Pros:

Memory and compute efficient

Works with small datasets

Lower risk of catastrophic forgetting

Cons:

May require careful tuning

Not always suitable for large-scale shifts

ğŸ§  Online Reinforcement Learning

The heart of post-training for models that learn why, not just what.

Why RL?

Like a child learning from trial and error, RL lets LLMs adapt based on outcomes and feedback.

ğŸŒ€ PPO (Proximal Policy Optimization)

Steps:

Model generates a response

A reward model scores it

Calculate "advantage"

Update policy accordingly

Pros:

High alignment with desired behaviors

Promotes deeper understanding

Cons:

Computationally expensive

Requires high-quality reward signals

ğŸ­ Reward Models

Reward Models assign scores to outputs. They are:

Often initialized from SFT-trained models

Fine-tuned using human preference data

Caution: Poor reward design (especially in math/code domains) can lead to unintended behavior.

ğŸ¨ Final Thoughts

Post-training is part science, part art. Itâ€™s how we:

Teach LLMs how to behave

Align them with values

Improve performance on real-world tasks

Summary of Techniques:
Technique	Purpose	Best For
SFT	Mimic ideal outputs	Quick prototyping
DPO	Align preferences	Behavioral correction
PEFT	Lightweight updates	Resource-constrained tuning
RL	Teach via feedback	Deep alignment
ğŸ“¦ Resources
GitHub Practice Notebooks

Hands-on code examples for:

GRPO

PPO

SFT

DPO

Access real-world Jupyter Notebooks to implement and experiment.

ğŸ“ DeepLearning.AI Course

â€œPost-training of LLMsâ€
Created with the University of Washington and NexusFlow.
A concise, effective course to dive deeper into post-training techniques.

ğŸ’¡ Your Challenge

Imagine a model tailored for your use case. Ask yourself:

Is SFT enough?

Should you design a reward function?

Could RL elevate your application?

Your next model might just be your most aligned one yet.

ğŸ‘¥ OWNER

https://www.deeplearning.ai/short-courses/post-training-of-llms/

Want to contribute? Open a PR or create an issue!

ğŸ“„ License

Distributed under the MIT License.
See LICENSE for more information.