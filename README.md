# ğŸš€ Post-Training of LLMs  
*Transforming Your Model from Ordinary to Extraordinary*  

Large Language Models (LLMs) started as simple next-word predictors.  
But what turned them into **assistants, teachers, coders, and creative collaborators**?  

ğŸ‘‰ **The answer: Post-training.**  

This repo explores the **key techniques** for refining pre-trained LLMs to make them **helpful, safe, and aligned**.  
While weâ€™ll overview several methods (SFT, DPO, PEFT), our **main focus** is **Online Reinforcement Learning (RL)** â€” the most dynamic path toward intelligent, value-aligned models.  

---

## ğŸ“š Table of Contents
- [ğŸ§  Introduction](#-introduction)  
- [ğŸ§± Why Post-Training?](#-why-post-training)  
- [ğŸ› ï¸ Techniques Overview](#ï¸-techniques-overview)  
  - [ğŸ¯ SFT â€” Supervised Fine-Tuning](#-sft--supervised-fine-tuning)  
  - [âš–ï¸ DPO â€” Direct Preference Optimization](#ï¸-dpo--direct-preference-optimization)  
  - [ğŸ§© PEFT â€” Parameter-Efficient Fine-Tuning](#-peft--parameter-efficient-fine-tuning)  
  - [ğŸ§  Online Reinforcement Learning](#-online-reinforcement-learning)  
- [ğŸ­ Reward Models](#-reward-models)  
- [ğŸ¨ Final Thoughts](#-final-thoughts)  
- [ğŸ“¦ Resources](#-resources)  
- [ğŸ‘¥ Owner](#-owner)  
- [ğŸ“„ License](#-license)  

---

## ğŸ§  Introduction
Post-training transforms a **raw LLM** into a **purpose-driven, value-aware assistant**.  

This repo covers:  
âœ”ï¸ The *why* behind post-training  
âœ”ï¸ Comparison of fine-tuning approaches  
âœ”ï¸ Deep dive into Reinforcement Learning  
âœ”ï¸ Hands-on resources & code  

---

## ğŸ§± Why Post-Training?
A pre-trained LLM understands **language**, but not **behavior, context, or correctness**.  

ğŸ‘‰ Think of it like onboarding a new employee: they *speak the language* but must learn the **company culture**.  

âŒ Without post-training, LLMs may:  
- Hallucinate or provide incorrect info  
- Miss user intent  
- Act unpredictably or unethically  

âœ… With post-training, LLMs:  
- Align with **human values**  
- Adapt to **domain-specific needs**  
- Improve **reliability & safety**  

---

## ğŸ› ï¸ Techniques Overview  

### ğŸ¯ SFT â€” Supervised Fine-Tuning  
Teach the model by **example** with inputâ€“output pairs.  
- **Pros:** Simple, fast, effective for new tasks  
- **Cons:** Limited generalization, dataset quality dependent  

---

### âš–ï¸ DPO â€” Direct Preference Optimization  
Instead of telling *what to do*, show **what not to do**.  
- **Pros:** Removes unwanted behavior, aligns preferences  
- **Cons:** Sensitive to bias, more complex  

---

### ğŸ§© PEFT â€” Parameter-Efficient Fine-Tuning  
Update **only parts of the model** for efficiency.  
- **Examples:** LoRA, Adapter Tuning  
- **Pros:** Memory-efficient, good for small data  
- **Cons:** Requires tuning, not always scalable  

---

### ğŸ§  Online Reinforcement Learning  
The **heart of post-training** â€” models learn *why*, not just *what*.  

**Process (PPO):**  
1. Model generates a response  
2. Reward model scores it  
3. Compute â€œadvantageâ€  
4. Update policy  

- **Pros:** Deep alignment, adaptive learning  
- **Cons:** Expensive, reward design critical  

---

## ğŸ­ Reward Models  
Reward models = *teachers* for LLMs.  
- Initialized from **SFT models**  
- Fine-tuned with **human preference data**  

âš ï¸ Poor design can cause failures (especially in math/code).  

---

## ğŸ¨ Final Thoughts  
Post-training is **science + art**:  
- Teaches behavior  
- Aligns values  
- Boosts real-world performance  

ğŸ“Š **Summary**  

| Technique | Purpose | Best For |
|-----------|---------|----------|
| SFT | Mimic ideal outputs | Quick prototyping |
| DPO | Align preferences | Behavioral correction |
| PEFT | Lightweight updates | Resource-constrained tuning |
| RL  | Learn via feedback | Deep alignment |

---

## ğŸ“¦ Resources  

ğŸ”¹ **GitHub Practice Notebooks**  
Hands-on code for GRPO, PPO, SFT, DPO  

ğŸ”¹ **ğŸ“ DeepLearning.AI Course**  
[Post-training of LLMs](https://www.deeplearning.ai/short-courses/post-training-of-llms/)  
Made with University of Washington & NexusFlow  

---

## ğŸ‘¥ Owner  
Repo maintained by: [DeepLearning.AI](https://www.deeplearning.ai/)  

Want to contribute? âœ¨ Open a PR or create an issue!  

---

## ğŸ“„ License  
Distributed under the **MIT License**. See `LICENSE` for details.  

---
